# heureka-greek-vases
Repository for Harvard CS 209B Project: Identifying and Locating Objects on Ancient Greek Vases

**Project Authors**: Abdulla Saif, Mark Conmy, Ibrahim Ouf, Lucas Kitzmüller<br/>

**The full code report is available [here](https://github.com/lucas-kitzmueller/heureka-greek-vases/blob/main/CS209B%20-%20Group%205%20-%20Identifying%20and%20Locating%20Objects%20on%20Ancient%20Greek%20Vases.html).**

<a id="part1-1"></a>
### Research Question 

There are massive quantities of images of Greek vases available to researchers. However, the **value of these databases remains limited if researchers have no practical way of searching and sorting these images.** For example, if a classicist who is interested in the representations of Dionysos or Herakles does not have an efficient method of compiling vases with images depicting these objects, they would miss out on a research opportunity. **Currently, labeling these images is a time-intensive manual process, and as a result, many images do not have labels or labels that provide insufficient details needed for more complex analyses.** 

**Developing machine learning models that detect whether and where objects of interest (e.g., Dionysos or Herakles) are depicted on a vase is a complex problem.** Greek vase painting is a uniform field with consistent representations of the objects. However, the **vases, and more importantly, the available images of the vases vary considerably** in terms of perspective, level of zoom, brightness, image dimensions, etc. In addition, computer vision to date has mostly focused on real-world images such as ImageNet. Therefore, learnings may not transfer perfectly to Greek vase paintings in which objects are shown in two dimensions using incisions (black figure vases) or lines (red figure vases).  

The original problem description is available [here](https://drive.google.com/drive/folders/1e9sQ9Q56F47q1n5AOlZktCDJnGe-FUtZ).

### 

We first **scraped the images and metadata from all collections in the [Arms and Armor database](http://armsandarmor.orphe.us/about).** This dataset includes 165,219 images from 66,649 vases. Based on an exploratory analysis of the existing labels, we **decided to focus on four objects: Dionysos, Herakles, Athena, and Hermes.** Given the low consistency and quality of images in the database, we then manually selected a subset of **consistent** images for training and testing. This process involved three steps: (i) **manually removing low quality images** (e.g., images that just show sketches), (ii) **manually creating four labels indicating whether the image depicts objects of interest,** and (iii) **applying a “smart-cropper” based on the Open Computer Vision package to standardize the paintings** and obtain as consistent a depiction as possible. The final dataset we used for training includes **2424 images in total** of which 1881 images depict at least one of the four objects.

We then **use the image labels (i.e. weakly supervised learning) to create a model that detects whether and where any of the objects are depicted on a vase.** Because of our limited number of training images, we rely on **transfer learning** — in particular using the Inception network as our backbone feature detector. Our architecture is based on the concept of utilizing **global average pooling (GAP)** for object detection, as described by [Zhou et al (2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf). **The models perform well in detecting whether objects are located on the image.** The F-1 scores of the predictions of our preferred model on the test data range from 0.38 (Hermes) to 0.8 (Dionysos). **The models perform less well in localizing objects on the images.** For tagging purposes, perhaps it is unnecessary to properly localize the objects, but doing so assists humans in confirming the model’s answers. The use of heatmaps could prove sufficient to assist in automatically labelling archives, but masks and bounding boxes rely on subjective cut-offs. **The model’s greatest strength is its sparse requirements for training, requiring to just know whether an object is somewhere on the image rather than where. This suits available data within the archives.**

We also **test using unsupervised learning**. In particular, we create **an introspective variational autoencoder (IntroVAE)** model as described by [Huang et al. 2019](https://arxiv.org/pdf/1807.06358.pdf). The motivation is that **if the autoencoder reconstructs images at high quality, we can apply a clustering algorithm to the latent space to detect emergent categories.** If the event classes/clusters align with meaningful interpretations (e.g., does the image show Dionysos?), then we could also generate vectors from that region in the latent space to create more datapoints for training. Unfortunately, **since the reconstructions are of poor quality, this approach did not work.** Tuning hyperparameters and expanding the dataset could still produce better results but both training the model and manually labelling images proved too time-intensive.
